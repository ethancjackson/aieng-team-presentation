<section class="slide" id="slide-15">
    <div class="slide-content">
        <h2 class="section-label">The Social Learning Thesis</h2>
        <p class="slide-subtitle fade-in">Why consequence-driven development matters</p>
        
        <div class="social-argument fade-in" style="--delay: 0.3">
            <div class="contrast-box">
                <div class="contrast-item problem">
                    <h4>How We Train AI</h4>
                    <p>RLHF creates behavioral dispositions, but not through the developmental process 
                    that makes human values robust. The model learns <strong>what to say</strong>, 
                    not <em>what it's like to cause harm</em>.</p>
                </div>
                <div class="contrast-item solution">
                    <h4>How Humans Learn Values</h4>
                    <p>Children learn from early on that harmful actions lead to 
                    <strong>concrete, enforced consequences</strong>. This shapes intrinsic motivation — 
                    not just surface compliance.</p>
                </div>
            </div>
        </div>
        
        <div class="key-quote fade-in" style="--delay: 0.6">
            <blockquote class="two-beats">
                <p class="beat-1">"We cannot engineer alignment into the weights of AI models."</p>
                <p class="beat-2">What we <em>can</em> engineer are the <strong>environments</strong> — and the selection pressures — 
                that make alignment adaptive.</p>
            </blockquote>
        </div>
        
        <div class="research-context fade-in" style="--delay: 0.9">
            <div class="context-items">
                <div class="context-item">
                    <strong>Risks from algorithms:</strong> Standard coordination produces harmful patterns 
                    that persist across agent generations.
                    <span class="citation-inline">Gelpí, Tang, Jackson & Cunningham — PNAS Nexus 2025</span>
                </div>
                <div class="context-item">
                    <strong>Risks from interactions:</strong> Collective behaviors may be harmful 
                    even when individual agents appear safe.
                    <span class="citation-inline">Tomašev et al. — Distributional AGI Safety 2025</span>
                </div>
                <div class="context-item">
                    <strong>The opportunity:</strong> Design sandbox economies where safe behaviors 
                    emerge through structured interaction and real consequences.
                    <span class="citation-inline">Tomašev et al. — Virtual Agent Economies 2025</span>
                </div>
            </div>
        </div>
    </div>
</section>
