=== SLIDE 0: TITLE ===

Neuroscience, Evolution, and the Path to Aligned AI
Research Introduction for the AI Engineering Team

Ethan C. Jackson, PhD
Applied ML Scientist, Vector Institute
January 12, 2026


=== SLIDE 1: ABOUT ME ===

Introduction

[FIGURE: Timeline with career milestones]

2014‚Äì2019: PhD Researcher, Western University
- Evolutionary RL / NAS, Novelty Search, Neuroimaging

2019‚Äì2020: Postdoctoral Researcher, University of Guelph
- Deep Learning for Forecasting, Canada's Food Price Report

2020‚Äì2022: Applied ML Scientist, Vector Institute
- Data Science & Forecasting, Privacy Technologies

2022‚Äì2024: Social AI Researcher, University of Toronto
- Multiagent RL, Artificial Hippocampus, Biologically Plausible Learning

2022‚Äì2025: Co-Founder, ChainML / Theoriq
- Analytics Agents, Memory & Learning, Agent-to-Agent Protocol

2026‚ÄìPresent: Applied ML Scientist, Vector Institute
- Agents & Reasoning, Social & Evolutionary AI


=== SLIDE 2: THREE RESEARCH THEMES ===

[FIGURE: Three cards with icons representing each theme]

üß† Neuroscience-Inspired AI
Leveraging brain insights to build better AI ‚Äî memory systems and learning algorithms

üß¨ Evolutionary Methods
LLM-guided program search and ES for fine-tuning ‚Äî evolutionary algorithms are powerful in the LLM era

üë• Social Learning Thesis
AI alignment may require agents that learn in environments with real consequences ‚Äî not just by RL after the fact

These aren't separate interests ‚Äî they represent a progression:
from how individuals learn, to how populations improve, to how groups coordinate.


=== SLIDE 3: THE MEMORY GAP ===

"A Definition of AGI" ‚Äî Hendrycks et al. (2025)
Framework based on Cattell-Horn-Carroll cognitive theory

[FIGURE: Bar chart showing GPT-4 at 27% and GPT-5 at 57% of AGI capability]

Cognitive Profile:
‚úì Reasoning
‚úì Knowledge
‚úì Perception
‚úì Processing Speed
‚úó Long-term Memory
‚úó Learning Efficiency
‚úó Experiential Memory

Primary gap: Memory systems ‚Äî and memory isn't just about capability.
It's about learning from consequences over time.


=== SLIDE 4: TRANSITION ===

What can neuroscience teach us about building memory systems for AI?


=== SLIDE 5: HIPPOCAMPUS OVERVIEW ===

Human Learning & Memory

[FIGURE: Diagram of hippocampal circuit showing:
- Cortex (distributed representations)
- EC (Entorhinal Cortex)
- DG (Dentate Gyrus) - 5% sparse
- CA3 (Pattern Completion)
- CA1 (Output)
- Arrows showing encoding and recall pathways]

The hippocampus balances pattern separation (distinct memories) with pattern completion (retrieval from cues)


=== SLIDE 6: PATTERN SEPARATION & COMPLETION ===

[FIGURE: Two-panel diagram]

Pattern Separation (Dentate Gyrus):
- Similar inputs ‚Üí distinct sparse codes
- Prevents memory interference
- Only ~5% of neurons active
- Visual: Two similar patterns A and B becoming orthogonal representations

Pattern Completion (CA3):
- Partial cue ‚Üí full memory recall
- Dense recurrent connections
- Associative retrieval
- Visual: Partial pattern reconstructed to complete memory

The tradeoff: Too much separation ‚Üí can't generalize. Too much completion ‚Üí memories blur.


=== SLIDE 7: HIPPOCAMPAL DNN ARCHITECTURE ===

Complementary Learning Systems: Fast hippocampal encoding + Slow neocortical consolidation

[FIGURE: Two pathways diagram]

MSP: Monosynaptic Pathway (ENCODING)
- Theta Phase 1 ‚Äî Direct autoencoder baseline (CA3 inhibited)
- EC ‚Üí slow ‚Üí CA1 ‚Üí slow ‚Üí EC

TSP: Trisynaptic Pathway (RECALL)
- Theta Phase 2 ‚Äî Pattern separation ‚Üí Association ‚Üí Completion
- EC ‚Üí DG ‚Üí fast ‚Üí CA3 ‚Üí fast ‚Üí CA1 ‚Üí EC

Fast learning ‚Äî One-shot episodic encoding (sparse ‚Üí low interference)
Slow learning ‚Äî Gradual semantic consolidation (distributed representations)


=== SLIDE 8: ENGINEERING EPISODIC MEMORY FOR AGENTS ===

We don't need hippocampal DNN modules to give agents episodic memory. RAG-based architectures offer practical engineering solutions ‚Äî with parallels to biological complementary learning systems.

[FIGURE: Two pipeline diagrams]

Memory Retrieval Pipeline (FAST RECALL):
Dialog ‚Üí LLM Query Extraction ‚Üí Vectorize ‚Üí pgvector Search ‚Üí Top-K Memories ‚Üí LLM Response

Memory Consolidation Pipeline (SLOW LEARNING):
Inputs ‚Üí LLM Analyze & Decide ‚Üí INSERT/UPDATE/DELETE ‚Üí Vectorize ‚Üí PostgreSQL (HNSW indexed)

Two distinct phases mirror biological memory: retrieval (pattern completion) and consolidation (post-interaction learning).

What would AGI benchmarks look like for a memory-enabled agent?


=== SLIDE 9: DISTRIBUTED TD LEARNING ===

Artificial Dopamine: Distributed TD Learning

[FIGURE: Neural network diagram showing:
- Three layers (1, 2, 3) at time t-1 and time t
- Each layer computes its own Q value (Q‚ÇÅ, Q‚ÇÇ, Q‚ÇÉ)
- Local TD errors (Œ¥‚ÇÅ, Œ¥‚ÇÇ, Œ¥‚ÇÉ) computed independently
- Reward broadcast to all layers
- Final QÃÑ(s,a) = Average of (Q‚ÇÅ+Q‚ÇÇ+Q‚ÇÉ)/3
- Action selection via argmax QÃÑ
- Arrows showing: Info UP (within timestep), Activations DOWN (across time)]

Guan, Verch, Voelcker, Jackson, Papernot & Cunningham ‚Äî NeurIPS 2024

Key insight: Each layer independently computes Œ¥ = r + Œ≥¬∑max Q(s',a') ‚àí Q(s,a) and updates its own weights. No error propagation between layers. No backprop (inference only).


=== SLIDE 10: TRANSITION ===

Memory helps agents learn from their own experience.

But what if we could learn from the experience of many agents at once?


=== SLIDE 11: EVOLUTIONARY PROGRAM SEARCH ===

LLM-driven evolution discovers algorithms by iteratively mutating, evaluating, and selecting programs

üß¨ Darwin G√∂del Machine ‚Äî Zheng, Hu, Lu, Lange & Clune (2025)
- Search space: Coding agent architectures
- Fitness: SWE-bench, Polyglot benchmarks
- Results: SWE-bench 20% ‚Üí 50%, Polyglot 14% ‚Üí 31%
- Self-modifying agents that improve their own code editing tools, context management, peer-review

üìà ProFiT ‚Äî Soper, Khalifa, Soros, Nasir, Azhang & Togelius (2025)
- Search space: Python trading programs
- Fitness: Walk-forward validation returns
- Results: 77% win rate vs Buy-Hold, 100% win rate vs Random
- Evolving strategies adapt to non-stationary markets via code mutation + self-analysis

[FIGURE: Three representations showing policies aren't just LLM prompts:
- üí¨ LLM Agent
- üß† Neural Net
- üìú Program]

Evolutionary search operates over any executable representation ‚Äî LLMs as mutation operators, not just as policies

Open-source tools like OpenEvolve make this accessible to try today


=== SLIDE 12: EVOLUTION STRATEGIES VS REINFORCEMENT LEARNING ===

ES scales to billions of parameters by exploring in parameter space, not action space

[FIGURE: Two-panel comparison]

Evolution Strategies (ES) ‚Äî Explore in parameter space:
- Visual: Population of 30 points (Œ∏‚ÇÅ...Œ∏‚ÇÉ‚ÇÄ) in parameter space, fitness-colored
- Œ∏ = Œ∏‚ÇÄ + œÉ¬∑(Œµ‚ÇÅ + Œµ‚ÇÇ + ¬∑¬∑¬∑ + Œµ‚Çú)
- Seeds ‚Üí Gaussian noise trajectory
- One noise sample per trajectory
- No backpropagation (inference only)
- Low variance, stable optimization
- No reward hacking observed
- Key trick: Store seeds, not tensors ‚Äî Memory: O(generations) vs O(7B params)

Reinforcement Learning (PPO/GRPO) ‚Äî Explore in action space:
- Visual: Token sequence "The ‚Üí answer ‚Üí is ‚Üí 42" with noise at every step
- Noise at every step ‚Üí high variance
- Requires backpropagation
- Credit assignment across 100s of tokens
- Prone to reward hacking (needs KL penalty)
- Empirical: 15.5√ó higher std across runs

Qiu, Gan, Hayes, Liang, Meyerson, Hodjat & Miikkulainen ‚Äî "Evolution Strategies at Scale: LLM Finetuning Beyond RL" (2025)


=== SLIDE 13: FROM GAME AGENTS TO LLM FINE-TUNING ===

The same evolutionary principles scale from millions to billions of parameters

[FIGURE: Timeline with three stages]

2017‚Äì2019: Game Agents üéÆ
- Atari DQN via GA, ~1M params, N=10,000+
- Such et al. (2017) ‚Üí Extended: Jackson (2019)

2025: LLM Fine-Tuning üß†
- Qwen/Llama 7B via ES, ~7B params, N=30 (!)
- Qiu et al. (2025)

Future: Agentic Systems? ü§ñ
- Full agent optimization
- Architecture + weights + prompts
- Open research direction

üî¨ Why Does N=30 Work?
[FIGURE: Diagram showing 7B params (apparent) mapping to ~1000 intrinsic dims]
Low intrinsic dimensionality ‚Äî LLMs' effective parameter space is much smaller than nominal. Consistent with LoRA.

üåä Landscape Smoothing
[FIGURE: Two loss landscapes - jagged RL (raw) vs smooth ES (smoothed)]
Gaussian convolution ‚Äî Population averaging smooths loss surface, enabling stable optimization.

Implication: Evolutionary methods may unlock optimization for agentic systems that gradient-based RL cannot ‚Äî architecture search, prompt evolution, multi-component agents.


=== SLIDE 14: TRANSITION ===

Evolution optimizes populations ‚Äî but each agent is evaluated independently.

What do we know about coordination, and cooperation in multi-agent systems?


=== SLIDE 15: THE SOCIAL LEARNING THESIS ===

Why consequence-driven development matters

[FIGURE: Two contrasting boxes]

How We Train AI:
RLHF creates behavioral dispositions, but not through the developmental process that makes human values robust. The model learns what to say, not what it's like to cause harm.

How Humans Learn Values:
Children learn from early on that harmful actions lead to concrete, enforced consequences. This shapes intrinsic motivation ‚Äî not just surface compliance.

"We cannot engineer alignment into the weights of AI models."

What we can engineer are the environments ‚Äî and the selection pressures ‚Äî that make alignment adaptive.

Risks from algorithms: Standard coordination produces harmful patterns that persist across agent generations.
‚Äî Gelp√≠, Tang, Jackson & Cunningham, PNAS Nexus 2025

Risks from interactions: Collective behaviors may be harmful even when individual agents appear safe.
‚Äî Toma≈°ev et al., Distributional AGI Safety 2025

The opportunity: Design sandbox economies where safe behaviors emerge through structured interaction and real consequences.
‚Äî Toma≈°ev et al., Virtual Agent Economies 2025


=== SLIDE 16: INTERESTING DIRECTIONS + CLOSING ===

A few areas I think are worth exploring

[FIGURE: Three cards]

üìä Agentic Evals
Most evals target models. We need benchmarks for agentic systems ‚Äî especially memory-augmented agents.
- Episodic memory benchmarks
- Multi-turn reasoning with context

üß¨ Evolutionary Methods
Evolutionary approaches for agent improvement ‚Äî optimizing weights or evolving code/scaffolding.
- ES for weight optimization
- Evolutionary program search (DGM-style)

üë• Social Learning
Building AI systems that develop intrinsic motivation through experience with real consequences.
- Consequence-aware training paradigms
- Multi-agent environments with accountability

Memory lets agents learn from their experiences. Evolution lets them learn from each other's attempts. Social structure lets them learn why cooperation matters. We need all three.

Thank you
ethan.jackson@vectorinstitute.ai
